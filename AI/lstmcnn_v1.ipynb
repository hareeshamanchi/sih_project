{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46208051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import os\n",
    "import math\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# GPU?\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bae37a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key loaded: a4003d7f23905ff2209adeff29fc386f\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load the .env file from the project root\n",
    "load_dotenv(dotenv_path=\"AI/.env\")  # adjust path if notebook is in /notebooks\n",
    "\n",
    "OPENWEATHER_API_KEY = os.getenv(\"OPENWEATHER_API_KEY\")\n",
    "print(\"API key loaded:\", OPENWEATHER_API_KEY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58cb17f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENWEATHER_API_KEY loaded: True\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# === CONFIG ===\n",
    "CSV_PATH = \"AI/synthetic_dataset (1).csv\"   # path to your CSV\n",
    "WINDOW = 14           \n",
    "BATCH_SIZE = 64\n",
    "LR = 1e-3\n",
    "EPOCHS = 80\n",
    "PATIENCE = 8         \n",
    "\n",
    "FEATURE_COLS = [\"temp\", \"humidity\", \"rainfall\", \"wqi\", \"diarrhea\", \"cholera\", \"typhoid\"]\n",
    "TARGET_COLS = [\"diarrhea\", \"cholera\", \"typhoid\"]\n",
    "MODEL_OUT = \"cnn_lstm_best.pth\"\n",
    "SEED = 42\n",
    "\n",
    "# ðŸ”‘ Load API key from .env file\n",
    "load_dotenv()\n",
    "OPENWEATHER_API_KEY = os.getenv(\"OPENWEATHER_API_KEY\")\n",
    "print(\"OPENWEATHER_API_KEY loaded:\", OPENWEATHER_API_KEY is not None)\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "356a426e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset preview:\n",
      "         Date       temp   humidity  rainfall        wqi  diarrhea  cholera  \\\n",
      "0  2024-01-01  19.461478  71.675727       0.0  51.414393        62       86   \n",
      "1  2024-01-02  22.791228  70.747452       0.0  51.734313        60       93   \n",
      "2  2024-01-03  19.489235  72.009741       0.0  49.689902        56       90   \n",
      "3  2024-01-04  22.852806  69.314989       0.0  48.437165        72       81   \n",
      "4  2024-01-05  22.272993  69.781259       0.0  47.146171        72      106   \n",
      "\n",
      "   typhoid  total_cases  temp_diff  humidity_diff  rainfall_diff  \n",
      "0       41          189   0.000000       0.000000            0.0  \n",
      "1       33          186   3.329750      -0.928276            0.0  \n",
      "2       40          186  -3.301993       1.262289            0.0  \n",
      "3       51          204   3.363571      -2.694752            0.0  \n",
      "4       46          224  -0.579813       0.466270            0.0  \n",
      "\n",
      "Feature columns: ['temp', 'humidity', 'rainfall', 'wqi', 'diarrhea', 'cholera', 'typhoid', 'temp_diff', 'humidity_diff', 'rainfall_diff']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\haree\\AppData\\Local\\Temp\\ipykernel_5052\\2086290342.py:2: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(CSV_PATH, parse_dates=[\"Date\"], dayfirst=True, infer_datetime_format=True)\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "df = pd.read_csv(CSV_PATH, parse_dates=[\"Date\"], dayfirst=True, infer_datetime_format=True)\n",
    "df = df.sort_values(\"Date\").reset_index(drop=True)\n",
    "\n",
    "df[FEATURE_COLS] = df[FEATURE_COLS].apply(pd.to_numeric, errors='coerce').fillna(0.0)\n",
    "df[TARGET_COLS] = df[TARGET_COLS].astype(int)\n",
    "\n",
    "# Add difference features\n",
    "for col in [\"temp\", \"humidity\", \"rainfall\"]:\n",
    "    df[f\"{col}_diff\"] = df[col].diff().fillna(0)\n",
    "\n",
    "FEATURE_COLS_EXT = FEATURE_COLS + [f\"{col}_diff\" for col in [\"temp\", \"humidity\", \"rainfall\"]]\n",
    "\n",
    "print(\"Dataset preview:\")\n",
    "print(df.head())\n",
    "print(\"\\nFeature columns:\", FEATURE_COLS_EXT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9e02dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, df, feature_cols, target_cols, window=WINDOW, scaler_x=None, scaler_y=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.feature_cols = feature_cols\n",
    "        self.target_cols = target_cols\n",
    "        self.window = window\n",
    "\n",
    "        X = self.df[self.feature_cols].values.astype(np.float32)\n",
    "        Y = self.df[self.target_cols].values.astype(np.float32)\n",
    "\n",
    "        self.scaler_x = MinMaxScaler() if scaler_x is None else scaler_x\n",
    "        self.scaler_y = MinMaxScaler() if scaler_y is None else scaler_y\n",
    "        if scaler_x is None:\n",
    "            self.scaler_x.fit(X)\n",
    "        if scaler_y is None:\n",
    "            self.scaler_y.fit(Y)\n",
    "\n",
    "        Xs = self.scaler_x.transform(X)\n",
    "        Ys = self.scaler_y.transform(Y)\n",
    "\n",
    "        self.samples = []\n",
    "        for i in range(window, len(self.df)):\n",
    "            x_win = Xs[i-window:i]\n",
    "            y_target = Ys[i]\n",
    "            self.samples.append((x_win.astype(np.float32), y_target.astype(np.float32)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.samples[idx]\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f2aac5d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset sizes: 8386 1800 1800\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "n = len(df)\n",
    "train_end = int(0.7 * n)\n",
    "val_end = int(0.85 * n)\n",
    "\n",
    "df_train = df.iloc[:train_end].reset_index(drop=True)\n",
    "df_val = df.iloc[train_end - WINDOW:val_end].reset_index(drop=True)\n",
    "df_test = df.iloc[val_end - WINDOW:].reset_index(drop=True)\n",
    "\n",
    "train_ds = SeqDataset(df_train, FEATURE_COLS_EXT, TARGET_COLS, window=WINDOW)\n",
    "val_ds = SeqDataset(df_val, FEATURE_COLS_EXT, TARGET_COLS, window=WINDOW,\n",
    "                    scaler_x=train_ds.scaler_x, scaler_y=train_ds.scaler_y)\n",
    "test_ds = SeqDataset(df_test, FEATURE_COLS_EXT, TARGET_COLS, window=WINDOW,\n",
    "                     scaler_x=train_ds.scaler_x, scaler_y=train_ds.scaler_y)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "\n",
    "print(\"Dataset sizes:\", len(train_ds), len(val_ds), len(test_ds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "570bbdc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNLSTM(\n",
      "  (conv1): Conv1d(10, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv3): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (bn3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU()\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (lstm): LSTM(256, 128, batch_first=True)\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=64, out_features=3, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "class CNNLSTM(nn.Module):\n",
    "    def __init__(self, n_features, cnn_channels=64, lstm_hidden=128, lstm_layers=1, out_dim=3, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(n_features, cnn_channels, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(cnn_channels)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(cnn_channels, cnn_channels*2, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(cnn_channels*2)\n",
    "\n",
    "        self.conv3 = nn.Conv1d(cnn_channels*2, cnn_channels*4, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm1d(cnn_channels*4)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=cnn_channels*4, hidden_size=lstm_hidden,\n",
    "                            num_layers=lstm_layers, batch_first=True)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(lstm_hidden, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64, out_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)   # (batch, features, seq)\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.relu(self.bn3(self.conv3(x)))\n",
    "        x = x.permute(0, 2, 1)   # (batch, seq, features)\n",
    "        x, _ = self.lstm(x)\n",
    "        out = x[:, -1, :]\n",
    "        return self.fc(out)\n",
    "\n",
    "model = CNNLSTM(n_features=len(FEATURE_COLS_EXT), out_dim=len(TARGET_COLS)).to(DEVICE)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "edfdbe8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train 0.028542, Val 0.015706\n",
      "  Saved best model at epoch 1\n",
      "Epoch 2: Train 0.019213, Val 0.015464\n",
      "  Saved best model at epoch 2\n",
      "Epoch 3: Train 0.018341, Val 0.015226\n",
      "  Saved best model at epoch 3\n",
      "Epoch 4: Train 0.017434, Val 0.015107\n",
      "  Saved best model at epoch 4\n",
      "Epoch 5: Train 0.016957, Val 0.015176\n",
      "Epoch 6: Train 0.016498, Val 0.015020\n",
      "  Saved best model at epoch 6\n",
      "Epoch 7: Train 0.016255, Val 0.015282\n",
      "Epoch 8: Train 0.015885, Val 0.015050\n",
      "Epoch 9: Train 0.015311, Val 0.015055\n",
      "Epoch 10: Train 0.015218, Val 0.015719\n",
      "Epoch 11: Train 0.014849, Val 0.015197\n",
      "Epoch 12: Train 0.014347, Val 0.015226\n",
      "Epoch 13: Train 0.014075, Val 0.015185\n",
      "Epoch 14: Train 0.014071, Val 0.015245\n",
      "Epoch 15: Train 0.013688, Val 0.015812\n",
      "Epoch 16: Train 0.013435, Val 0.015470\n",
      "Epoch 17: Train 0.012874, Val 0.015568\n",
      "Epoch 18: Train 0.012399, Val 0.015834\n",
      "Epoch 19: Train 0.012008, Val 0.015880\n",
      "Epoch 20: Train 0.011823, Val 0.016169\n",
      "Epoch 21: Train 0.011516, Val 0.016230\n",
      "Epoch 22: Train 0.010885, Val 0.016838\n",
      "Epoch 23: Train 0.010608, Val 0.017074\n",
      "Epoch 24: Train 0.010360, Val 0.017066\n",
      "Epoch 25: Train 0.010070, Val 0.017535\n",
      "Epoch 26: Train 0.009890, Val 0.017193\n",
      "Epoch 27: Train 0.009453, Val 0.017666\n",
      "Epoch 28: Train 0.009245, Val 0.017560\n",
      "Epoch 29: Train 0.009047, Val 0.018065\n",
      "Epoch 30: Train 0.008995, Val 0.017632\n",
      "Epoch 31: Train 0.008888, Val 0.017744\n",
      "Epoch 32: Train 0.008666, Val 0.018039\n",
      "Epoch 33: Train 0.008524, Val 0.018364\n",
      "Epoch 34: Train 0.008563, Val 0.018205\n",
      "Epoch 35: Train 0.008417, Val 0.018125\n",
      "Epoch 36: Train 0.008419, Val 0.018259\n",
      "Epoch 37: Train 0.008191, Val 0.018414\n",
      "Epoch 38: Train 0.008230, Val 0.018453\n",
      "Epoch 39: Train 0.008154, Val 0.018441\n",
      "Epoch 40: Train 0.008147, Val 0.018425\n",
      "Epoch 41: Train 0.008054, Val 0.018499\n",
      "Epoch 42: Train 0.008051, Val 0.018513\n",
      "Epoch 43: Train 0.007950, Val 0.018569\n",
      "Epoch 44: Train 0.008002, Val 0.018480\n",
      "Epoch 45: Train 0.007974, Val 0.018614\n",
      "Epoch 46: Train 0.007969, Val 0.018589\n",
      "Epoch 47: Train 0.007953, Val 0.018566\n",
      "Epoch 48: Train 0.007946, Val 0.018445\n",
      "Epoch 49: Train 0.007943, Val 0.018584\n",
      "Epoch 50: Train 0.007892, Val 0.018560\n",
      "Epoch 51: Train 0.007904, Val 0.018655\n",
      "Epoch 52: Train 0.007948, Val 0.018616\n",
      "Epoch 53: Train 0.007807, Val 0.018629\n",
      "Epoch 54: Train 0.007872, Val 0.018602\n",
      "Epoch 55: Train 0.007864, Val 0.018653\n",
      "Epoch 56: Train 0.007833, Val 0.018585\n",
      "Epoch 57: Train 0.007789, Val 0.018644\n",
      "Epoch 58: Train 0.007909, Val 0.018633\n",
      "Epoch 59: Train 0.007820, Val 0.018613\n",
      "Epoch 60: Train 0.007819, Val 0.018638\n",
      "Epoch 61: Train 0.007797, Val 0.018672\n",
      "Epoch 62: Train 0.007837, Val 0.018666\n",
      "Epoch 63: Train 0.007841, Val 0.018610\n",
      "Epoch 64: Train 0.007820, Val 0.018698\n",
      "Epoch 65: Train 0.007807, Val 0.018632\n",
      "Epoch 66: Train 0.007945, Val 0.018707\n",
      "Epoch 67: Train 0.007823, Val 0.018667\n",
      "Epoch 68: Train 0.007799, Val 0.018707\n",
      "Epoch 69: Train 0.007805, Val 0.018673\n",
      "Epoch 70: Train 0.007804, Val 0.018736\n",
      "Epoch 71: Train 0.007843, Val 0.018722\n",
      "Epoch 72: Train 0.007869, Val 0.018692\n",
      "Epoch 73: Train 0.007865, Val 0.018625\n",
      "Epoch 74: Train 0.007782, Val 0.018647\n",
      "Epoch 75: Train 0.007769, Val 0.018707\n",
      "Epoch 76: Train 0.007890, Val 0.018622\n",
      "Epoch 77: Train 0.007868, Val 0.018669\n",
      "Epoch 78: Train 0.007805, Val 0.018674\n",
      "Epoch 79: Train 0.007818, Val 0.018644\n",
      "Epoch 80: Train 0.007809, Val 0.018716\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "def train_model_full_epochs(model, train_loader, val_loader, epochs=EPOCHS, lr=LR):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=4)\n",
    "    best_val = 1e9\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        for X, y in train_loader:\n",
    "            X, y = X.to(DEVICE), y.to(DEVICE)\n",
    "            pred = model(X)\n",
    "            loss = criterion(pred, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        with torch.no_grad():\n",
    "            for Xv, yv in val_loader:\n",
    "                Xv, yv = Xv.to(DEVICE), yv.to(DEVICE)\n",
    "                pv = model(Xv)\n",
    "                val_losses.append(criterion(pv, yv).item())\n",
    "\n",
    "        train_loss, val_loss = np.mean(train_losses), np.mean(val_losses)\n",
    "        scheduler.step(val_loss)\n",
    "        print(f\"Epoch {epoch}: Train {train_loss:.6f}, Val {val_loss:.6f}\")\n",
    "\n",
    "        # Save model if validation improves\n",
    "        if val_loss < best_val - 1e-6:\n",
    "            best_val = val_loss\n",
    "            torch.save(model.state_dict(), MODEL_OUT)\n",
    "            print(f\"  Saved best model at epoch {epoch}\")\n",
    "\n",
    "    model.load_state_dict(torch.load(MODEL_OUT, map_location=DEVICE))\n",
    "    return model\n",
    "\n",
    "# Train model for all epochs\n",
    "model = train_model_full_epochs(model, train_loader, val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "39763afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def classify_risk(disease, value):\n",
    "    thresholds = {\n",
    "        \"diarrhea\": {\"low\": 30, \"medium\": 60},\n",
    "        \"cholera\": {\"low\": 50, \"medium\": 80},\n",
    "        \"typhoid\": {\"low\": 20, \"medium\": 40}\n",
    "    }\n",
    "    \n",
    "    if value <= thresholds[disease][\"low\"]:\n",
    "        return \"LOW âœ… â€” normal levels\"\n",
    "    elif value <= thresholds[disease][\"medium\"]:\n",
    "        return \"MEDIUM âš ï¸ â€” monitor closely\"\n",
    "    else:\n",
    "        return \"HIGH ðŸš¨ â€” take preventive measures\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "63086354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def predict_next_day(model, df_recent, lat, lon):\n",
    "    \"\"\"Use live weather + model to predict next-day diseases\"\"\"\n",
    "    try:\n",
    "        weather_data = requests.get(\n",
    "            f\"https://api.openweathermap.org/data/2.5/weather?lat={lat}&lon={lon}&appid={OPENWEATHER_API_KEY}&units=metric\"\n",
    "        ).json()\n",
    "        if \"main\" not in weather_data:\n",
    "            raise ValueError(\"Weather API returned no 'main'\")\n",
    "        live_weather = {\n",
    "            'temp': round(weather_data['main']['temp'], 2),\n",
    "            'humidity': round(weather_data['main']['humidity'], 2),\n",
    "            'rainfall': round(weather_data.get('rain', {}).get('1h', 0), 2)\n",
    "        }\n",
    "        print(\"Weather API raw response:\", weather_data)\n",
    "    except Exception as e:\n",
    "        print(\"âš ï¸ Weather API failed:\", e)\n",
    "        live_weather = {\n",
    "            'temp': df_recent['temp'].iloc[-1],\n",
    "            'humidity': df_recent['humidity'].iloc[-1],\n",
    "            'rainfall': df_recent['rainfall'].iloc[-1]\n",
    "        }\n",
    "\n",
    "    df_next = df_recent.copy()\n",
    "    df_next['temp_diff'] = df_next['temp'].diff().fillna(0)\n",
    "    df_next['humidity_diff'] = df_next['humidity'].diff().fillna(0)\n",
    "    df_next['rainfall_diff'] = df_next['rainfall'].diff().fillna(0)\n",
    "\n",
    "    df_next.iloc[-1, df_next.columns.get_loc('temp')] = live_weather['temp']\n",
    "    df_next.iloc[-1, df_next.columns.get_loc('humidity')] = live_weather['humidity']\n",
    "    df_next.iloc[-1, df_next.columns.get_loc('rainfall')] = live_weather['rainfall']\n",
    "\n",
    "    df_next.iloc[-1, df_next.columns.get_loc('temp_diff')] = df_next['temp'].iloc[-1] - df_next['temp'].iloc[-2]\n",
    "    df_next.iloc[-1, df_next.columns.get_loc('humidity_diff')] = df_next['humidity'].iloc[-1] - df_next['humidity'].iloc[-2]\n",
    "    df_next.iloc[-1, df_next.columns.get_loc('rainfall_diff')] = df_next['rainfall'].iloc[-1] - df_next['rainfall'].iloc[-2]\n",
    "\n",
    "    X_next = df_next[FEATURE_COLS_EXT].values.astype(np.float32)\n",
    "    X_next_scaled = train_ds.scaler_x.transform(X_next)\n",
    "    X_next_tensor = torch.tensor(X_next_scaled).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred_scaled = model(X_next_tensor).cpu().numpy()\n",
    "\n",
    "    pred = train_ds.scaler_y.inverse_transform(pred_scaled)[0]\n",
    "    # Round predictions to integer\n",
    "    pred_int = [int(round(p)) for p in pred]\n",
    "    result = {col: pred_int[i] for i, col in enumerate(TARGET_COLS)}\n",
    "    result.update(live_weather)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b5acecfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weather API raw response: {'coord': {'lon': 78.2624, 'lat': 13.6576}, 'weather': [{'id': 804, 'main': 'Clouds', 'description': 'overcast clouds', 'icon': '04n'}], 'base': 'stations', 'main': {'temp': 22.44, 'feels_like': 22.93, 'temp_min': 22.44, 'temp_max': 22.44, 'pressure': 1010, 'humidity': 84, 'sea_level': 1010, 'grnd_level': 930}, 'visibility': 10000, 'wind': {'speed': 5.14, 'deg': 283, 'gust': 12.49}, 'clouds': {'all': 100}, 'dt': 1758907385, 'sys': {'country': 'IN', 'sunrise': 1758846965, 'sunset': 1758890433}, 'timezone': 19800, 'id': 1264621, 'name': 'Madanapalle', 'cod': 200}\n",
      "ðŸŒ Next-day Disease Forecast with Risk Alerts\n",
      "Diarrhea: 68 cases â†’ HIGH ðŸš¨ â€” take preventive measures\n",
      "Cholera: 96 cases â†’ HIGH ðŸš¨ â€” take preventive measures\n",
      "Typhoid: 45 cases â†’ HIGH ðŸš¨ â€” take preventive measures\n",
      "\n",
      "Weather Conditions: ðŸŒ¡ï¸ 22.44Â°C, ðŸ’§ 84%, ðŸŒ§ï¸ 0mm\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "recent_df = df.iloc[-WINDOW:].copy()\n",
    "next_day_pred = predict_next_day(model, recent_df, lat=13.6576 , lon=78.2624)\n",
    "\n",
    "print(\"ðŸŒ Next-day Disease Forecast with Risk Alerts\")\n",
    "for disease in [\"diarrhea\", \"cholera\", \"typhoid\"]:\n",
    "    value = next_day_pred[disease]\n",
    "    alert = classify_risk(disease, value)\n",
    "    print(f\"{disease.capitalize()}: {value} cases â†’ {alert}\")\n",
    "\n",
    "print(f\"\\nWeather Conditions: ðŸŒ¡ï¸ {next_day_pred['temp']}Â°C, ðŸ’§ {next_day_pred['humidity']}%, ðŸŒ§ï¸ {next_day_pred['rainfall']}mm\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
